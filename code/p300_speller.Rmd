---
title: "p300_speller"
author: "Christopher W Bishop"
date: "Thursday, January 01, 2015"
output: html_document
---

This is a Markdown document for the P300 speller competition hosted on Kaggle (<http://www.kaggle.com/c/inria-bci-challenge>).

Data Preprocessing
----------------

Electroencephalography data were processed primarily in MATLAB 2013b using EEGlab v12.0.2.5b and ERPLAB 4.0.3.1. Below is a summary of the data processing steps.

1. Data were band pass filtered from 0.05 - 20 Hz using a 4th order IIR (Butter).    
    + The low pass cutoff (20 Hz) was used to remove prominent, high-amplitude noise at ~50 Hz. 
    + 0.05 Hz high pass was used to remove low-frequency drifts, but had little practical effect on the time series.

2. Feedback events were used as event triggers and data were epoched (segmented) about each feedback event for -50 ms to 1800 ms. 

3. Each data epoch was then baseline corrected using the pre-feedback baseline.

4. Feedback events were sorted by whether they were correct or incorrect. 

5. 340 sweeps per subject were exported for further analysis in R (see below)

Set Working Directory, Libraries, and Constants
----------------
```{r}
setwd("D:/GitHub/Kaggle/code")
source('D:/GitHub/Kaggle/code/p300_speller.R')
```

Loading the Data
----------------

Data were converted from CSV to RData format using import_eeg.csv. All subsequent data analysis will use the RData files.

The code below loads the RData files and generates group ERPs for the training and test sets.

```{r}
data.train <- import_eeg.rdata(subjects.train, erp_label)
data.test <- import_eeg.rdata(subjects.test, erp_label)

erp.train <- make.erp(data.train)
erp.test <- make.erp(data.test)
```

A First Look athe Data
----------------

ERPLAB was used to do most of the data visualization, but below is a cursory summary of observations.

1. Filtering can have a significant impact on the discriminability between correct and incorrect feedback events
    + The original paper describing this work used a 1 - 20 Hz band pass filter. This removes what is clearly the most discriminable feature of the set: a huge difference from ~500 - 1600 ms. 

2. There are three potentially useful gross features in the ERP spatiotemporal waveforms that can be used to distinguish between correct and incorrect feedback events.
    + A difference between ~220 and 320 ms. This is approximately equivalent to the "Neg-ErrP" described in the seminal paper.
    + A difference between ~320 and 440 ms. This is approximately equivalent to the "Pos-ErrP" described in the seminal paper
    + A large, sustained difference from ~440 - 1600 ms over virtually all electrode sites
        + The distributed topography is likely due to the nose reference. 
        
Here is a brief look at these differences at Cz (Electrode 29). Cz was selected to ease comparisons with the seminal paper, although the reader should recall that the filters differ substantially and thus the time waveforms may look very different.

```{r}
plot(erp.train$time_stamps, erp.train$erp_incorrect[29,], col = 'red', lwd = 2, type = 'l')
lines(erp.train$time_stamps, erp.train$erp_correct[29,], col = 'black', lwd = 2)
lines(erp.train$time_stamps, erp.train$erp_incorrect[29,] - erp.train$erp_correct[29,], col = 'blue', lwd = 2)
```

Notice that there are three distinct time periods during which differences can be seen across (almost) all electrodes. The largest difference appears to also be the latest.

Let's take a first pass at the data using a simple, binary classifier.

Pearson's Correlation Coefficient
----------------

Considering the striking difference in spatio-temporal pattern between correct and incorrect feedback events, I thought it worthwhile to use perhaps the simplest classifier imaginable on the data to start. My hope was to verify that what is reflected in the group-level ERPs is in fact useful for predicting single-trial sweeps for a single subject.

So, I started with Pearson's Correlation Coefficient (PCC) of the spatio-temporal data.

To verify that the (super simple) classifier is doing what it's intended to do (or at least approximately so), I ran a leave-one-out analysis on the training set. 

```{r}


pcc.results <- classify.pcc(data.train, data.test)
pcc.eval <- classifier.eval(pcc.results$label.cross_val, pcc.results$prediction.cross_val)

# Accuracy plot
data2plot <- as.data.frame(cbind((subjects.train), (pcc.eval$accuracy)))
names(data2plot) <- c("subject", "accuracy")
ggplot(data2plot, aes(subject, accuracy)) + geom_point(size = 5)
summary(t(pcc.eval$accuracy))

# Sensitivity plot
data2plot <- as.data.frame(cbind((subjects.train), (pcc.eval$sensitivity)))
names(data2plot) <- c("subject", "sensitivity")
ggplot(data2plot, aes(subject, sensitivity)) + geom_point(size = 5)
summary(t(pcc.eval$sensitivity))

# Specificity plot
data2plot <- as.data.frame(cbind((subjects.train), (pcc.eval$specificity)))
names(data2plot) <- c("subject", "specificity")
ggplot(data2plot, aes(subject, specificity)) + geom_point(size = 5)
summary(t(pcc.eval$specificity))
```
Notice that the (ultra simple) classifier does "OK" for some subjects (some accuracy as high as 64%), but definitely not for all subjects. 

CWB wondered what's so different about the poor performing subjects? Let's look at the lowest and highest performing subjects in the training set (cross-validation data).

```{r}

# First, calculate the difference wave since this is an informative and concise metric.
erp.difference <- matrix(nrow = length(data.train), ncol = length(data.train[[1]]$erp_correct))
for(i in 1:length(data.train)){
    erp.difference[i,] <- data.train[[i]]$erp_incorrect - data.train[[i]]$erp_correct
}

# Best performer (S24)
subjects.train[which.max(pcc.eval$accuracy)]

# Worst performer (S12)
subjects.train[which.min(pcc.eval$accuracy)]

# Get plot data
data2plot <- data.frame(data.train[[1]]$time_stamps, colMeans(erp.difference), erp.difference[which.max(pcc.eval$accuracy),], erp.difference[which.min(pcc.eval$accuracy),])

# Assign names
names(data2plot) <- c("time_ms", "Average", "Best", "Worst")

# Create a new plot with this information
gg <- ggplot(data2plot, aes(x = time_ms, y = Average)) + geom_point(colour = 'black', size = 2) + geom_point(aes(x = time_ms, y = Best), colour = 'green') + geom_point(aes(x = time_ms, y = Worst), colour = 'red')
gg
```

Notice that the best performer's difference wave looks a heck of a lot like the group difference wave. In contrast, the poorest performer's difference wave actually has the sign REVERSED compared to the group average. If we could predict the sign of the difference wave, our performance would impprove substantially, I suspect.

How do we go about predicting the sign of the difference wave? This is relatively easy for the labeled data - we can just test for it explicitly. It would be much more difficult in the test set (unlabeled). Perhaps we can leverage the fact that the feedback is correct a majority of the time for most subjects? If we use an unsupervised clustering algorithm (e.g., K-means) to determine how many sweeps cluster into each category, then label the cluster with the most sweeps as "correct", we should be able to predict the sign of the difference wave with some level of certainty. 

If a more complex, unsupervised classification scheme cannot deal with the sign differences, then we'll likely need to focus in on more reliable information. CWB took a look at the same data, but with a 1 Hz high-pass filter and noted that the responses seemed to be much more consistent between s12, s24, and the group average. Perhaps removing the low-frequency information is a reasonable way to go? We could try it and run it through the same PCC-like classifier. 

Additionally, we could downsample the data and use a more flexible classifier, like logistic regression with a variable decision boundary. That's where CWB would like to go next, he thinks. 

1 to 20 Hz Bandpass Filter
-------

Let's take a look at a similar data set that has been bandpass filtered from 1 to 20 Hz. The high pass filter was used to filter out some of the low-frequency drift that (apparently) is not terribly reliable across subjects. 

For starters, let's try running the same PCC based classifier, but with the different filter settings. This will give me a chance to do a head to head comparison while just changing the filter order. Perhaps the classifier will work better without the low frequency information. 

```{r}

# Change the ERP Label to match the newly filtered data set. 
erp_label <- '-erp_filtered_1to20_epoched_-50to1800'

# Load the training data
data.test <- data.test <- import_eeg.rdata(subjects.test, erp_label)

# Create a group ERP

```

Feature Filtering
--------

Since we know the spatio-temporal pattern has *some* useful information in it, we may be able to improve performance by limiting our correlations to time frames during which there is a statistically significant difference (that is, a consistent difference) between correct and incorrect feedback trials.

One approach to limiting the test space is to filter our spatiotemporal features with something like a t-test. Below, we attempt to do exactly that

Let's first gather the difference wave (erp_incorrect - erp_correct) into a useful variable. Each row is a subject, each column is a spatiotemporal data point.

The t-test filtering was largely unhelpful with the 0.05 to 20 Hz bandpass filtered data. So I won't bother to show the results here. 


